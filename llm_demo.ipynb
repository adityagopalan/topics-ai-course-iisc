{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6908fd3d4fe49edb8a1d713f91e3cc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define the model (llama2)\n",
    "\n",
    "model = \"meta-llama/Llama-2-7b-hf\" # replace with llama-2-7b-chat-hf for chatbot style model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    # device=0 # for using the GPU\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you know the height of Mount Everest? 8,848 metres. The highest mountain on Earth, it is located in the Himalayan mountain range on the border between Nepal and Tibet, China. The Himalayas are also home to the highest mountain range in\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'Do you know the height of Mount Everest? ',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_new_tokens=50,\n",
    "    temperature=1.0,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"{seq['generated_text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: Tell me a story about a person writing a tutorial for installing llama 2 where every letter starts with s\n",
      "I'm not sure what that means, but I'm going to tell you a story about a person writing a tutorial for installing llama 2 where every letter starts with s.\n",
      "I'm not sure what that means, but I'm going to tell you a story about a person writing a tutorial for installing llama 2 where every letter starts with s. I'm not sure what that means, but I'm going to tell you a story about a person writing a tutorial for installing llama 2 where every letter starts with s. I'm not sure what that means, but I'm going to tell you a story about a person writing a tutorial for installing llama 2 where every letter starts with s. I'm not sure what that means, but I'm going to tell you a story about a person writing a\n"
     ]
    }
   ],
   "source": [
    "sequences = pipeline(\n",
    "    'Tell me a story about a person writing a tutorial for installing llama 2 where every letter starts with s\\n',\n",
    "    do_sample=True,\n",
    "    top_k=10,\n",
    "    num_return_sequences=1,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    max_length=200,\n",
    ")\n",
    "for seq in sequences:\n",
    "    print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
